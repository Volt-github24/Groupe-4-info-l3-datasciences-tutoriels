{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutoriel sur le perceptron multicouche (5).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volt-github24/Groupe-4-info-l3-datasciences-tutoriels/blob/main/Tutoriel_sur_le_perceptron_multicouche_(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFOL3 - SDD- UY1 - Departement d'informatique**"
      ],
      "metadata": {
        "id": "v5Kn8thuMPF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTATION DU TUTORIEL SUR LE PERCEPTRON MULTICOUCHE**\n",
        "\n",
        " >>>  [*Optimizing Neural Networks using Keras (with Image recognition case study)*](https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/#one)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                   \n",
        "### Noms des membres du groupe (**Groupe 4**) :\n",
        "  1. NOUCHEN TCHAMBA Parnell Voltaire - 19M2326\n",
        "  2. DJEUMEZA DONGMO Julie Merveille - 19M2105\n",
        "  3. MADJOU Alvine Patricia - 19M2103\n",
        "  4. NAKAM YOPDUP Manuella Kristeva - 19M2233"
      ],
      "metadata": {
        "id": "VhWSdGjDMRX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iMPLEMENTATON DE **KERAS** SUR UN DATASET "
      ],
      "metadata": {
        "id": "6l-XjSn-okQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout d'abord il est necessaire d'etablir une liaison avec le drive pour pouvoiraccederaux fichiersa utiliser"
      ],
      "metadata": {
        "id": "B-7zH9xSV7R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YgCZ5I4UMsaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e097f3-4db3-46e4-c401-af563007022b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importation des differentes librairies"
      ],
      "metadata": {
        "id": "MYcCh68FWuB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_fWH5D1HHq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142c6bc4-2f72-4de4-90fb-e266c83808b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialisation du RandomState"
      ],
      "metadata": {
        "id": "SSQWUBgTXMC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To stop potential randomness\n",
        "seed = 128\n",
        "rng = np.random.RandomState(seed)"
      ],
      "metadata": {
        "id": "Q4kq5pkJKFeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "definition des chemins d'acces"
      ],
      "metadata": {
        "id": "LjcbHLmYK-Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MyDrive/ML\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OODxHnStnb0L",
        "outputId": "cd1b30e1-3911-4a38-f551-f80640f47d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt90ZueWnizz",
        "outputId": "58338747-8404-4a64-b93f-d39c4775bd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mtest\u001b[0m/  Test.csv  \u001b[01;34mtrain\u001b[0m/  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = os.path.abspath('../..') \n",
        "data_dir = os.path.join(root_dir, 'data') \n",
        "sub_dir = os.path.join(root_dir, 'sub') \n",
        "# check for existence \n",
        "os.path.exists(root_dir) \n",
        "os.path.exists(data_dir) \n",
        "os.path.exists(sub_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RIR-OBlKFNW",
        "outputId": "90a7decc-aa5a-4455-a761-9388e73f1a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "g8sWogtcU50w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "db3b0956-048c-4c7d-bc17-5c685bbd22e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  filename  label\n",
              "0    0.png      4\n",
              "1    1.png      9\n",
              "2    2.png      1\n",
              "3    3.png      7\n",
              "4    4.png      3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2b637fb-9f18-49b5-9189-6e38f7c7b96e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.png</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.png</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.png</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.png</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.png</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2b637fb-9f18-49b5-9189-6e38f7c7b96e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2b637fb-9f18-49b5-9189-6e38f7c7b96e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2b637fb-9f18-49b5-9189-6e38f7c7b96e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "print(img_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "BWTYGA_yhbie",
        "outputId": "07e4ff75-93b4-401e-fb94-5e7f49eec42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  Test.csv\ttrain  train.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1de9e826b11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'img_name' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = rng.choice(train.filename)\n",
        "filepath = os.path.join('train', img_name)\n",
        "\n",
        "img = imread(filepath, flatten=True)\n",
        "\n",
        "pylab.imshow(img, cmap='gray')\n",
        "pylab.axis('off')\n",
        "pylab.show()"
      ],
      "metadata": {
        "id": "twnBfIA3VU_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "972884ae-028a-4443-abdc-c325a49a152a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-eed25b515f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/content/drive/MyDrive/ML/train/23635.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = []\n",
        "for img_name in train.filename:\n",
        "    image_path = os.path.join(data_dir, 'Train', 'Images', 'train', img_name)\n",
        "    img = imread(image_path, flatten=True)\n",
        "    img = img.astype('float32')\n",
        "    temp.append(img)\n",
        "    \n",
        "train_x = np.stack(temp)\n",
        "\n",
        "train_x /= 255.0\n",
        "train_x = train_x.reshape(-1, 784).astype('float32')\n",
        "\n",
        "temp = []\n",
        "for img_name in test.filename:\n",
        "    image_path = os.path.join(data_dir, 'Train', 'Images', 'test', img_name)\n",
        "    img = imread(image_path, flatten=True)\n",
        "    img = img.astype('float32')\n",
        "    temp.append(img)\n",
        "    \n",
        "test_x = np.stack(temp)\n",
        "\n",
        "test_x /= 255.0\n",
        "test_x = test_x.reshape(-1, 784).astype('float32')"
      ],
      "metadata": {
        "id": "EWsigCPR2EWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "2d9ffc6e-b52b-4af7-e23f-596cad1f77d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5034601fadda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA2d4tTFMywS",
        "outputId": "371b5379-8da2-428b-d512-59a5ac19f088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQW84P9zN5hp",
        "outputId": "7c899038-1475-4af8-b126-9fef95c9b60b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd/drive/Mydrive/data_perceptron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUs_2BEPN_sq",
        "outputId": "863d5bfc-050d-4394-9916-f041d88526f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/drive/Mydrive/data_perceptron'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd/drive/Mydrive/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulcsxUmka_H2",
        "outputId": "8e7cfe13-a063-4ac9-fda4-32726ed190f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/drive/Mydrive/train'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd/drive/Mydrive/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLUVgKo7iuMq",
        "outputId": "404bd061-2b30-4d5f-b2cb-b56e99f0ce5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/drive/Mydrive/test'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd/drive/Mydrive/data_perceptron_multicouches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJh5CWmv7vPR",
        "outputId": "43104aba-b3e8-424e-fb31-bcfc4e3aa658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/drive/Mydrive/data_perceptron_multicouches'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import tensorflow.keras import layers\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "# Definition du nombre de neuronnes d'entrees\n",
        "input_num_units = 784\n",
        "\n",
        "# Definition de 5 couches cachees avec 50 neuronnes associees a chacune des couches cachees\n",
        "hidden1_num_units = 50\n",
        "hidden2_num_units = 50\n",
        "hidden3_num_units = 50\n",
        "hidden4_num_units = 50\n",
        "hidden5_num_units = 50\n",
        "\n",
        "# Definition de nombre de neuronnes en sortie a 10\n",
        "output_num_units = 10\n",
        "\n",
        "# Definition du nombre d'epochs, ie le nombre de fois que l'on va faire passer l'ensemble du jeu de donnees\n",
        "epochs = 5\n",
        "\n",
        "#Definition de la taille d'un lot, ie du nombre de couches par lots\n",
        "batch_size = 128\n",
        "\n",
        "# Definition d'un modele sequentiel \n",
        "model = keras.Sequential([\n",
        "layer.Dense(784, activation='relu', name='layer1'),\n",
        "layer.Dense(50, activation='relu', name='layer2'), \n",
        "layer.Dense(50, activation='relu', name='layer3'), \n",
        "layer.Dense(50, activation='relu', name='layer4'),  \n",
        "layer.Dense(50, activation='relu', name='layer5'), \n",
        "layer.Dense(50, activation='relu', name='layer6'), \n",
        "\n",
        "layer.Dense(10, activation='softmax', name'layer7'),\n",
        "])"
      ],
      "metadata": {
        "id": "HwnPlcCh9F8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "87c5eabc-f2ba-4c98-8ec5-5b1955f2b243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['imread']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3f9669d38235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Definition d'un modele sequentiel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m model = Sequential([\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden1_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden2_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden1_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden3_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden2_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/core/dense.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m                **kwargs):\n\u001b[1;32m    111\u001b[0m     super(Dense, self).__init__(\n\u001b[0;32m--> 112\u001b[0;31m         activity_regularizer=activity_regularizer, **kwargs)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     }\n\u001b[1;32m    340\u001b[0m     \u001b[0;31m# Validate optional keyword arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m   1172\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'output_dim')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropery', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "trained_model_5d_with_drop = model.fit(train_x, train_y, nb_epoch=epochs, batch_size = batch_size, validation = (val_x, val_y))"
      ],
      "metadata": {
        "id": "oEmxum7np-Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition des entrees\n",
        "input_num_units = 784\n",
        "\n",
        "# Definition de 5 couches cachees avec 50 neuronnes associees a chacune des couches cachees\n",
        "hidden1_num_units = 50\n",
        "hidden2_num_units = 50\n",
        "hidden3_num_units = 50\n",
        "hidden4_num_units = 50\n",
        "hidden5_num_units = 50\n",
        "\n",
        "# Definition de nombre de couches en sortie a 10\n",
        "output_num_units = 10\n",
        "\n",
        "# Definition du nombre d'epochs, ie le nombre de fois que l'on va faire passer l'ensemble du jeu de donnees\n",
        "epochs = 5\n",
        "\n",
        "#Definition de la taille d'un lot, ie du nombre de couches par lots\n",
        "batch_size = 128\n",
        "\n",
        "dropout_ratio = 0.2\n",
        "\n",
        "# Definition d'un modele sequentiel \n",
        "model = Sequential([\n",
        "Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio),\n",
        "Dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='relu'), \n",
        "Dropout(dropout_ratio),\n",
        "Dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio),\n",
        "Dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio), \n",
        "Dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio),\n",
        "\n",
        "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
        "])"
      ],
      "metadata": {
        "id": "mbjN_U-JtpQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropery', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "trained_model_5d_with_drop = model.fit(train_x, train_y, nb_epoch=epochs, batch_size = batch_size, validation = (val_x, val_y))"
      ],
      "metadata": {
        "id": "wcx8Y_h24rXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_num_units = 784\n",
        "\n",
        "# Definition de 5 couches cachees avec 50 neuronnes associees a chacune des couches cachees\n",
        "hidden1_num_units = 50\n",
        "hidden2_num_units = 50\n",
        "hidden3_num_units = 50\n",
        "hidden4_num_units = 50\n",
        "hidden5_num_units = 50\n",
        "\n",
        "# Definition de nombre de couches en sortie a 10\n",
        "output_num_units = 10\n",
        "\n",
        "# Definition du nombre d'epochs, ie le nombre de fois que l'on va faire passer l'ensemble du jeu de donnees\n",
        "epochs = 50\n",
        "\n",
        "#Definition de la taille d'un lot, ie du nombre de couches par lots\n",
        "batch_size = 128\n",
        "\n",
        "dropout_ratio = 0.2\n",
        "\n",
        "# Definition d'un modele sequentiel \n",
        "model = Sequential([\n",
        "Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio),\n",
        "Dense(output_dim=hidden2_num_units, input_dim=hidden1_num_units, activation='relu'), \n",
        "Dropout(dropout_ratio),\n",
        "Dense(output_dim=hidden3_num_units, input_dim=hidden2_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio),\n",
        "Dense(output_dim=hidden4_num_units, input_dim=hidden3_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio), \n",
        "Dense(output_dim=hidden5_num_units, input_dim=hidden4_num_units, activation='relu'),\n",
        "Dropout(dropout_ratio),\n",
        "\n",
        "Dense(output_dim=output_num_units, input_dim=hidden5_num_units, activation='softmax'),\n",
        "])"
      ],
      "metadata": {
        "id": "vgF2EIid47ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Une fois le nombre d'epochs ajuste, verifions l=notre precision\n",
        "model.compile(loss='categorical_crossentropery', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "trained_model_5d_with_drop = model.fit(train_x, train_y, nb_epoch=epochs, batch_size = batch_size, validation = (val_x, val_y))"
      ],
      "metadata": {
        "id": "ijvg7S_p5OAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PEAUFINEMENT** **DU** **MODELE** **DE** **RESEAU** **DE** **NEURONES**"
      ],
      "metadata": {
        "id": "n1sAR0yejCdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importation des modules supplementaires à ceux importer plus haut\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Convolution2D, Flatten, MaxPooling2D, Reshape, InputLayer"
      ],
      "metadata": {
        "id": "k13_E1706bWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On genere des nombres aléatoires\n",
        "\n",
        "seed = 128 #initialisation du generateur de nombres aleatoires\n",
        "rng = np.random.RandomState(seed)"
      ],
      "metadata": {
        "id": "CC87fA1BUIiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#On définit les differents chemins qui nous seront utiles\n",
        "\n",
        "root_dir = os.path.abspath('../..')\n",
        "data_dir = os.path.join(root_dir, 'data')\n",
        "sub_dir = os.path.join(root_dir, 'sub')\n",
        "\n",
        "# Verification de leur exixtence\n",
        "os.path.exists(root_dir)\n",
        "os.path.exists(data_dir)\n",
        "os.path.exists(sub_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPtBcx2oUJ0N",
        "outputId": "935a81fd-7a6e-4586-8bde-b342a18d3529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lecture de l'ensemble des donnees et convertion sous forme utilisable\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/data_perceptron/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/data_perceptron/Test.csv')\n",
        "\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/data_perceptron/sample_submission.csv')\n",
        "\n",
        "temp = []\n",
        "for img_name in train.filename:\n",
        "  if img_name == \"101.png\":\n",
        "      break\n",
        "  image_path = \"/content/drive/MyDrive/data_perceptron/Images/train/\"+img_name #On recupere le chemin d'acces pour chaque image dans le fichier d'entrainement\n",
        "  img = imread(image_path) # Chargement de l'image d'entrainement\n",
        "  img = img.astype('float32')\n",
        "  temp.append(img) # ajout de l'image d'entrainement dans temp\n",
        "\n",
        "train_x = np.stack(temp)\n",
        "\n",
        "train_x /= 255.0\n",
        "train_x = train_x.reshape(-1, 784).astype('float32')\n",
        "\n",
        "temp = []\n",
        "for img_name in test.filename:\n",
        "  if img_name == \"49101.png\":\n",
        "      break\n",
        "  image_path = \"/content/drive/MyDrive/data_perceptron/Images/test/\"+img_name#On recupere le chemin d'acces pour chaque image dans le fichier de test\n",
        "  img = imread(image_path) # Chargement de l'image de test\n",
        "  img = img.astype('float32')\n",
        "  temp.append(img) # ajout de l'image de test dans temp\n",
        "\n",
        "test_x = np.stack(temp)\n",
        "\n",
        "test_x /= 255.0\n",
        "test_x = test_x.reshape(-1, 784).astype('float32')\n",
        "\n",
        "train_y = keras.utils.np_utils.to_categorical(train.label.values)"
      ],
      "metadata": {
        "id": "n3ObQo86pEwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Division du jeu d'entrainement en jeu de formation  et jeu de validation\n",
        "\n",
        "split_size = int(train_x.shape[0]*0.7)\n",
        "\n",
        "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
        "train_y, val_y = train_y[:split_size], train_y[split_size:]"
      ],
      "metadata": {
        "id": "2LF_NGs0UyWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition des variables\n",
        "\n",
        "input_num_units = 784 #nombre de neurone en entree\n",
        "hidden_num_units = 500 # Ici le nombre de neurone dans la couche cachee a ete augmentee et est fixé à 500  \n",
        "output_num_units = 10 # nombre de neurones en sortie\n",
        "epochs = 5 # nombre d'epochs\n",
        "batch_size = 128 # definition de la taille d'un lot\n",
        "\n",
        "#Definition du modele sequentiel\n",
        "model = Sequential([\n",
        " Dense(output_dim=hidden_num_units, input_dim=input_num_units, activation='relu'),\n",
        " Dense(output_dim=output_num_units, input_dim=hidden_num_units, activation='softmax'),\n",
        "])"
      ],
      "metadata": {
        "id": "W7wywnKPVDu1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "b7ed05cf-49e0-4b03-9a9c-e4887594dece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-30987a46df62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#Definition du modele sequentiel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m model = Sequential([\n\u001b[0;32m---> 11\u001b[0;31m  \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m  \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_num_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m ])\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testons le modele\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "trained_model_500 = model.fit(train_x, train_y, nb_epoch=epochs, batch_size=batch_size, validation_data=(val_x, val_y))"
      ],
      "metadata": {
        "id": "rqTYYniLVJTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "35QnKXS7T6Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mise en œuvre de réseaux de neurones à l'aide de TensorFlow**"
      ],
      "metadata": {
        "id": "Jq6R7sHAvMTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant ecrivons un petit programme permenttant d'additionner deux nombres avec tensorflow"
      ],
      "metadata": {
        "id": "1vgS_4pmvnuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# build computational graph\n",
        "a = tf.compat.v1.placeholder(tf.int16)\n",
        "b = tf.compat.v1.placeholder(tf.int16)\n",
        "\n",
        "addition = tf.add(a, b)\n",
        "\n",
        "# initialize variables\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "# create session and run the graph\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init)\n",
        "    print(\"Addition: %i\" % sess.run(addition, feed_dict={a: 5, b: 3}))\n",
        "\n",
        "# close session\n",
        "sess.close()\n"
      ],
      "metadata": {
        "id": "LgPtwtlVvwTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'approche de tensorflow nous avons:\n",
        "Crée un graphique de calcul, il peut s'agit ici de l'addition comme operation.\n",
        "Initialiser les variables, pour compiler les variables définies précédemment\n",
        "Créer une session, le graphe compilé est passé à la session, qui démarre son exécution, ensuite on ferme la session."
      ],
      "metadata": {
        "id": "4qDNsO-SVBeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passons à l'mplementation du reseau de neuronnes.\n",
        "\n",
        "- Définition de l'architecture du réseau de neurones à compiler\n",
        "- Transfért des données vers notre modèle\n",
        "- Sous le capot, les données sont d'abord divisées en lots, afin de pouvoir être ingérées. Les lots sont d'abord prétraités, augmentés puis introduits dans le réseau de neurones pour la formation\n",
        "- Le modèle est ensuite entraîné progressivement\n",
        "- Affichage de la précision pour un nombre spécifique de pas de temps\n",
        "- Après la formation, enregistrons le modèle pour une utilisation future\n",
        "- Testons le modèle sur de nouvelles données et vérifions ses performances"
      ],
      "metadata": {
        "id": "YhtI_hQtVEbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iMPORTATION DES MODULES NECESSAIRES\n",
        "%pylab inline\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imageio import imread\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Definissons en quelque sorte la graine pour le random\n",
        "seed = 128\n",
        "rng = np.random.RandomState(seed)\n",
        "\n",
        "\n",
        "#Importons les donnnees\n",
        "train = pd.read_csv('/content/drive/MyDrive/data_perceptron/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/data_perceptron/Test.csv')\n",
        "\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/data_perceptron/sample_submission.csv')\n",
        "\n",
        "train.head() # pour voir a quoi ressemblent les 05 premieres lignes du dataset train\n",
        "\n",
        "# Lisons une image et affichons pour voir à quoi elles ressemblent.\n",
        "\n",
        "img = imread('/content/drive/MyDrive/data_perceptron/Images/train/1.png') # cest une image prise dans le lot d'images d'entrainement\n",
        "\n",
        "pylab.imshow(img, cmap='gray')\n",
        "pylab.axis('off')\n",
        "pylab.show()\n",
        "\n",
        "print(\"La matrice correspondante à cette image \\n \\n\", img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dDXfUaFeVHzu",
        "outputId": "79f56877-b098-426e-9d0f-22191e8f2e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['seed', 'imread']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La matrice correspondante à cette image \n",
            " \n",
            " [[[  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  ...\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]]\n",
            "\n",
            " [[  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  ...\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]]\n",
            "\n",
            " [[  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  ...\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  ...\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]]\n",
            "\n",
            " [[  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  ...\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]]\n",
            "\n",
            " [[  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  ...\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]\n",
            "  [  0   0   0 255]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour une manipulation plus aisee, mettons toutes ces donnees dans une matrice numpy"
      ],
      "metadata": {
        "id": "xmRBJd4HVTbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = []\n",
        "for img_name in train.filename:\n",
        "    if img_name == \"101.png\":\n",
        "      break\n",
        "    image_path = \"/content/drive/MyDrive/data_perceptron/Images/train/\"+img_name\n",
        "    img = imread(image_path)\n",
        "    img = img.astype('float32')\n",
        "    temp.append(img)\n",
        "    \n",
        "train_x = np.stack(temp)\n",
        "\n",
        "temp = []\n",
        "for img_name in test.filename:\n",
        "    if img_name == \"49101.png\":\n",
        "      break\n",
        "    image_path = \"/content/drive/MyDrive/data_perceptron/Images/test/\"+img_name\n",
        "    img = imread(image_path)\n",
        "    img = img.astype('float32')\n",
        "    temp.append(img)\n",
        "    \n",
        "test_x = np.stack(temp)"
      ],
      "metadata": {
        "id": "jZqq-wUevRbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour tester le bon fonctionnement de notre modèle, nous créons un ensemble de validation. Prenons une taille fractionnée de 70:30 pour l'ensemble de train par rapport à l'ensemble de validation"
      ],
      "metadata": {
        "id": "3xbsoxebVojl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_size = int(train_x.shape[0]*0.7)\n",
        "\n",
        "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
        "train_y, val_y = train.label.values[:split_size], train.label.values[split_size:]"
      ],
      "metadata": {
        "id": "mLgetXvrVptT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant definissons quelques fonctions que nous utiliserons plutard"
      ],
      "metadata": {
        "id": "tDbMgygoVvJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_to_one_hot(labels_dense, num_classes=10):\n",
        "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
        "    num_labels = labels_dense.shape[0]\n",
        "    index_offset = np.arange(num_labels) * num_classes\n",
        "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
        "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
        "    \n",
        "    return labels_one_hot\n",
        "\n",
        "def preproc(unclean_batch_x):\n",
        "    \"\"\"Convert values to range 0-1\"\"\"\n",
        "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
        "    \n",
        "    return temp_batch\n",
        "\n",
        "def batch_creator(batch_size, dataset_length, dataset_name):\n",
        "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
        "    batch_mask = rng.choice(dataset_length, batch_size)\n",
        "    \n",
        "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
        "    batch_x = preproc(batch_x)\n",
        "    \n",
        "    if dataset_name == 'train':\n",
        "        batch_y = eval(dataset_name).ix[batch_mask, 'label'].values\n",
        "        batch_y = dense_to_one_hot(batch_y)\n",
        "        \n",
        "    return batch_x, batch_y"
      ],
      "metadata": {
        "id": "SHL7IwGnVwMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Définissons notre architecture de réseau de neurones. Nous définissons un réseau de neurones à 3 couches : entrée, caché et sortie. Le nombre de neurones en entrees et en sortie est fixe car nous connaissons la taille d'une image et le nombre de classse en sortie (respectivement 28*28 et 10)"
      ],
      "metadata": {
        "id": "to2R2EbZWG-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "### set all variables\n",
        "\n",
        "# number of neurons in each layer\n",
        "input_num_units = 28*28 # 28*28 neurones en entrees\n",
        "hidden_num_units = 500 # 500 neurones dans la couche intermediare\n",
        "output_num_units = 10 # 10 neurones pour la sortie pour les 10 classes que nous avons\n",
        "\n",
        "# define placeholders\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, input_num_units])\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, output_num_units])\n",
        "\n",
        "# set remaining variables\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "\n",
        "### define weights and biases of the neural network \n",
        "\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random.normal([input_num_units, hidden_num_units], seed=seed)),\n",
        "    'output': tf.Variable(tf.random.normal([hidden_num_units, output_num_units], seed=seed))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random.normal([hidden_num_units], seed=seed)),\n",
        "    'output': tf.Variable(tf.random.normal([output_num_units], seed=seed))\n",
        "}"
      ],
      "metadata": {
        "id": "6chl_SmAWQdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creons  maintenant notre graphe de calcul de réseaux de neurones"
      ],
      "metadata": {
        "id": "DwFST_exWXrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "\n",
        "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']"
      ],
      "metadata": {
        "id": "gTG3HzNFWYxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definissons le cout de notre reseau de neurones"
      ],
      "metadata": {
        "id": "xVogMWBfWh8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_layer, y))\n"
      ],
      "metadata": {
        "id": "m2NCEqUpWhe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous utiliserons une variante de l'algorithme de descente de gradient : ADAM pour minimiser le cout"
      ],
      "metadata": {
        "id": "VPcb2vUPWotY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "metadata": {
        "id": "6ZwakjUCWqG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que la structure de notre reseau de neurones est ok, definissons nos variables."
      ],
      "metadata": {
        "id": "bl8u9QtVbBeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init = tf.compat.v1.global_variables_initializer()"
      ],
      "metadata": {
        "id": "C_19_atHbKv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créons maintenant une session et exécutons notre réseau de neurones dans la session. Nous validons également la précision de nos modèles sur l'ensemble de validation que nous avons créé"
      ],
      "metadata": {
        "id": "h8WA2XSZdZuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "    # create initialized variables\n",
        "    sess.run(init)\n",
        "    \n",
        "    ### for each epoch, do:\n",
        "    ###   for each batch, do:\n",
        "    ###     create pre-processed batch\n",
        "    ###     run optimizer by feeding batch\n",
        "    ###     find cost and reiterate to minimize\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        avg_cost = 0\n",
        "        total_batch = int(train.shape[0]/batch_size)\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
        "            c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
        "            avg_cost += c / total_batch\n",
        "        print (\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
        "    print (\"\\nTraining complete!\")\n",
        "    # find predictions on val set\n",
        "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
        "    print(\"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}))\n",
        "    \n",
        "    predict = tf.argmax(output_layer, 1)\n",
        "    pred = predict.eval({x: test_x.reshape(-1, input_num_units)})"
      ],
      "metadata": {
        "id": "MwUC5p-rdaUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testons le modele"
      ],
      "metadata": {
        "id": "rnCmW_VCdiZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = rng.choice(test.filename)\n",
        "filepath = '/content/drive/MyDrive/data_perceptron/Images/Test/49050.png'\n",
        "\n",
        "img = imread(filepath)\n",
        "\n",
        "test_index = int(img_name.split('.')[0]) - 49000\n",
        "\n",
        "print (\"Prediction is: \", pred[test_index])\n",
        "\n",
        "pylab.imshow(img, cmap='gray')\n",
        "pylab.axis('off')\n",
        "pylab.show()"
      ],
      "metadata": {
        "id": "vkdzlxypdxp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pMUDjLmVt02D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HgWsWIvjt0p9"
      }
    }
  ]
}