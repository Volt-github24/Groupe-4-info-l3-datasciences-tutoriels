{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ngrams tests 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiw71qA29eKvvYxonWB6mw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Volt-github24/Groupe-4-info-l3-datasciences-tutoriels/blob/main/ngrams_tests_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4DBNA_qMStM",
        "outputId": "7c6b6162-0c37-4acb-eacc-50acbb356486"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "import os\n",
        "os.chdir(\"/content/drive/\")\n",
        "%cd MyDrive/Collab_Stage_L3/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF1MrCHEUngA",
        "outputId": "c6cd3b65-a0c1-4821-9f2e-b0894dea0fcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Collab_Stage_L3\n",
            "'Copie de Rapport sur les NLG.gdoc'\n",
            " corpus.txt\n",
            "'Generation de langage naturel.ipynb'\n",
            " Prise_en_main_Spacy.ipynb\n",
            "\"Rapport d'implementation du tutoriel sur les NLG.gdoc\"\n",
            "'Test Ngram corpus francais.ipynb'\n",
            " textgenrnn_weights.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction de tokenisation pour tokeniser le texte d'entree\n"
      ],
      "metadata": {
        "id": "DXppPcIoGHWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# Fonction de Tokenisation\n",
        "\n",
        "import spacy # importation de la librairie\n",
        "!python -m spacy download fr_core_news_sm # pipeline pour le francais\n",
        "nlp = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "def return_token(sentence):\n",
        "    # Tokeniser la phrase\n",
        "    doc = nlp(sentence)\n",
        "    # Retourner le texte de chaque token\n",
        "    return [X.text for X in doc]    \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXQP8Q2GFfUN",
        "outputId": "0a2e3ef6-c95c-48d5-ac86-81e226801d82"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-05 13:21:19.059343: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 24.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation du texte  (corpus)"
      ],
      "metadata": {
        "id": "wo2ri7iMGaP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# Corpus avec ponctuation\n",
        "liste = []\n",
        "with open(\"corpus.txt\", \"r\", encoding='latin-1') as f:\n",
        "  for line in f.readlines():\n",
        "    liste.append(line)\n",
        "\n",
        "# Tokenisation du corpus\n",
        "corpus = [] # corpus est la liste des tokens\n",
        "for line in liste:\n",
        "  result = return_token(line)[:-1]\n",
        "  corpus.append(result)\n",
        "\n",
        "punc = [':', \"!\", \"^\", '.', '`', '~', ';', ',', '?', '...', '_']"
      ],
      "metadata": {
        "id": "hamdWXBWGdq5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, je teste une nouvelle approche pour les ngrams, ainsi donc, le principe est que pour chaque sequence de mot, il faut savoir ses differents successeurs, cest a dire que quand j'ai une sequence de mots, je dois avoir un tableau qui me montre tous les successeurs possibles (a partir du corpus fournit ) pour cette sequence de mots, apres ce la, comme la machine ne comprend que les nombres, on va constituer un dictionnaire (mot : index) et un dictionnaire (index : mot) pour avoir l'index d'un mot et par la meme occasion le mot correspondant à un index"
      ],
      "metadata": {
        "id": "VfAClG3IsuWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNIGRAMS**"
      ],
      "metadata": {
        "id": "4S1RGMqzm2Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# UNIGRAMS\n",
        "\n",
        "#Ici il s'agit du cas selon lequel la sequence de mot est un seul mot.\n",
        "#construisons donc les dictionnaires en question  \n",
        "\n",
        "def create_word_index_unigram(liste_tokens):\n",
        "    word_to_index = {\n",
        "    \"START\":0,\n",
        "    \"END\":1\n",
        "    }\n",
        "    \n",
        "    index_to_word = ['START', 'END']\n",
        "\n",
        "    for speech in liste_tokens:\n",
        "        for word in speech: # je parcours chaque phrase  je parcours ses tokens\n",
        "            if word not in index_to_word: # et je n''ajoute un token que s'il n'est pas deja dans la liste des tokens \n",
        "                word_to_index[word] = len(index_to_word) # et dans le dictionnaire, la valeur pour la cle qui est le mot, est simplement a position de ce mot dans la liste des tokens\n",
        "                #print(len(index_to_word))\n",
        "                index_to_word.append(word) # ensuite j'ajoute ce token\n",
        "                \n",
        "    return word_to_index, index_to_word # et je retourne le dictionnaire et la liste\n",
        "\n",
        "# dictionnaire, liste = create_word_index_unigram(liste_tokens)"
      ],
      "metadata": {
        "id": "KftQLYEiqpx9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construisons à présent la matrice mots:successeurs"
      ],
      "metadata": {
        "id": "TP6yyBVkNXf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def create_unigram_matrix(word_to_index, index_to_word, texts):\n",
        "    V = len(index_to_word) #taille de la matrice, la matrice est de v*v, v etant le nombre de mots differents qu'il ya dans le corpus'\n",
        "    matrix = np.zeros((V, V)) # je cree la matrice en question\n",
        "\n",
        "    for sentence in texts: #je parcours chaque phrase du corpus\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0:\n",
        "                matrix[0, word_to_index[sentence[i]]] += 1 # Si le mot est le premier d'une phrase, je met la colonne de START à 1, pour marque que cest le debut d'une phrase\n",
        "            else: \n",
        "                matrix[word_to_index[sentence[i-1]], word_to_index[sentence[i]]] += 1 # dans le cas ou il est un mot dans une phrase, j'incremenete la case mot successeur\n",
        "\n",
        "        if i == (len(sentence)-1):\n",
        "            matrix[word_to_index[sentence[i]], 1] += 1 # Si le mot est le dernier d'une phrase, je met la colonne de END à 1, pour marque que cest la fin d'une phrase\n",
        "\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "pUWzkgRKLVcg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Get word to index and index to word for unigrams\n",
        "word_to_index_unigram, index_to_word_unigram = create_word_index_unigram(corpus)\n",
        "\n",
        "#Get unigram matrix\n",
        "unigram_matrix = create_unigram_matrix(word_to_index_unigram,  index_to_word_unigram, corpus)"
      ],
      "metadata": {
        "id": "9pmKNezJS86T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour la suite essayons de generer le texte(completer) avec cette matice construite sur la base de unigrams"
      ],
      "metadata": {
        "id": "nCVHxhmnPPyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction pour retrouver le mot suivant :  il prend l'index du mot et retourne l'index du mot suivant"
      ],
      "metadata": {
        "id": "HVsGnt9Qge_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "def get_next_word(index_previous_ngram, matrix, show_nb_possibilities=False): # il prend l'index du mot et retourne l'index du mot suivant\n",
        "    '''\n",
        "    index_previous_ngram : int, represent the word index in the word_to_index matrix\n",
        "    matrix : successor matrix, lines represent the n-gram, columns represent the successor\n",
        "\n",
        "    1. Check how many successors are possible\n",
        "    2. Select a random word from the possible words, to avoid repetition\n",
        "    '''\n",
        "    \n",
        "    if (matrix[index_previous_ngram]>0).sum() < 2:\n",
        "        nb_possible = 1\n",
        "    elif (matrix[index_previous_ngram]>0).sum() > 9:\n",
        "        nb_possible = 10\n",
        "    else:\n",
        "        nb_possible = (matrix[index_previous_ngram]>0).sum()\n",
        "\n",
        "    if show_nb_possibilities==True:\n",
        "        print('NUMBER POSSIBLE ', nb_possible)\n",
        "\n",
        "    top_indexes = matrix[index_previous_ngram].argsort()[-nb_possible:] # constitue la liste des 10 mots suivants les plus succeptibles (avec le plus d'occurence) , 10 etant le nombre de possibiltes, il varie en trre 1 et 10\n",
        "    random_index = math.floor(random.random()*nb_possible) # juste pour choisir aleatoirement  l'index du prochin mot parmi toutes les possibilites\n",
        "\n",
        "    index_next_word = top_indexes[random_index] # on choisi un index aleatoirement dans cet ensemble des 10 possibilites\n",
        "    \n",
        "    if matrix[index_previous_ngram][index_next_word] == 0: # Si aucun mot n'est successeur, je renvoie le END\n",
        "      index_next_word = 1 # C'est l'indice du END\n",
        "\n",
        "    return index_next_word, matrix[index_previous_ngram][index_next_word]  # retourne l'index du prochain mot, et le nombre d'occurence de ce prochain mot sachant le mot precedent dans le corpus"
      ],
      "metadata": {
        "id": "1TQzWXFnVJtG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction pour compteter le texte avec unigram"
      ],
      "metadata": {
        "id": "19YM0yQAgmnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_unigram(previous_text, word_to_index, index_to_word,  matrix, nb_words_after=100, no_repetition=True):\n",
        "\n",
        "    #1. tokenisation de la phrse en entree\n",
        "    used_words = return_token(previous_text)\n",
        "\n",
        "    #2. Recuperons le dernier mot de la demie phrase\n",
        "    last_word = used_words[-1]\n",
        "\n",
        "    print(previous_text, end=\" \")\n",
        "    index_last_word = word_to_index[last_word] # on recupere l'index du dernier mot de la demie phrase\n",
        "    \n",
        "    for i in range(nb_words_after):      #  cette boucle est la pour que si le break du bas ne s'execute pas, qu'on complete au plus le nombre de mots precisé\n",
        "        index_last_word, occurence = get_next_word(index_last_word, matrix)\n",
        "                \n",
        "        if(index_last_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word[index_last_word])\n",
        "          print(index_to_word[index_last_word], end = ' ') # j'affiche donc au fur et à mesure sans aller à la ligne\n",
        "        else:\n",
        "            break;\n",
        "\n",
        "            \n",
        "\n",
        "get_sentence_unigram(\"ENEO\", word_to_index_unigram, index_to_word_unigram,unigram_matrix,nb_words_after=10)"
      ],
      "metadata": {
        "id": "eNhmofnSgpq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AU final, avec les unigrams, le modele fait tellement d'erreurs car il ne tient pas compte du passe lointain de la phrase a completer, essayons donc les bigrams pour ameliorer les resultats\n",
        " Cest a dire on considere pour sequence de mots (deux mots)"
      ],
      "metadata": {
        "id": "0ae11feYlqMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BIGRAMS**"
      ],
      "metadata": {
        "id": "-5KxnLAHmwOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout d'abord definissons d'abor les dictionnaires comme dans la methode unigram"
      ],
      "metadata": {
        "id": "T0VJjxaYnU4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_index_bigram(texts):\n",
        "    word_to_index = {\n",
        "    \"START START\":0,\n",
        "    \"END\":1\n",
        "    }\n",
        "    index_to_word = ['START START', 'END'] # exactement comme plus haut, sauf qu'ici les sequences sont constituees de deux mots, raison pour laquelle on a START START\n",
        "\n",
        "    for speech in texts:\n",
        "        for i in range(len(speech)):\n",
        "            if i == 0:\n",
        "                word = 'START ' + speech[0] # si je suis avec le premier mot d'une phrse pour stocker cette sequence, je concatene START avec ce premier mot\n",
        "            else:\n",
        "                word = speech[i-1] + ' ' + speech[i] # si cest dans la phrase, je le prend juste avec son precedent\n",
        "\n",
        "            if word not in index_to_word:\n",
        "                word_to_index[word] = len(index_to_word) # j'ajoute dans le dictionnaire \n",
        "                index_to_word.append(word) # et dans la liste , si biensur la sequence n'y etait pas encore\n",
        "            \n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "def create_bigram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts): # elle prend le dictionnaire des bigrams, la liste des bigrams, le dictionnaire des unigram, car les mots suivant sont un sequence d'un seul mot, bien evidement et le texte corpus aussi\n",
        "    V = len(index_to_word)\n",
        "    V_unigram = len(word_to_index_unigram)\n",
        "    matrix = np.zeros((V, V_unigram)) # on cree la matrice de taille , nombre de bigrams, nombre d'unigrams, les unigrams sont les successeurs\n",
        "\n",
        "    for sentence in texts:\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0: # cas ou je suis avec le premier mot de la phrase\n",
        "                matrix[word_to_index['START START'], word_to_index_unigram[sentence[0]]] += 1            # pour marquer le debut de la phrase\n",
        "                if len(sentence)>1:\n",
        "                  matrix[word_to_index['START '+sentence[0]], word_to_index_unigram[sentence[1]]] += 1      # pour marquer le debut de la phrase\n",
        "\n",
        "            elif i >= (len(sentence)-1):\n",
        "                matrix[word_to_index[sentence[i-1]+' '+sentence[i]], 1] += 1 # si je ne suis pas dans le debut de la phrase, mais au dernier mot plutot je met plutot la colonne de END  a 1 pour marquer la fin de la phrase.\n",
        "       \n",
        "            else: \n",
        "                matrix[word_to_index[sentence[i-1]+' '+sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1 # si je suis au milieu de la phrase, eh bien j'uncrement simplement la case correspondante au successeur\n",
        "    \n",
        "    return matrix"
      ],
      "metadata": {
        "id": "99sKikSvmyEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word to index and index to word for bigrams\n",
        "word_to_index_bigram, index_to_word_bigram = create_word_index_bigram(corpus)\n",
        "\n",
        "#Get bigram matrix\n",
        "bigram_matrix = create_bigram_matrix(word_to_index_bigram, index_to_word_bigram, word_to_index_unigram, corpus)\n",
        "\n",
        "index_to_word_bigram[:20]"
      ],
      "metadata": {
        "id": "u1lW6tB3Mbw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generons maintenant le texte avec le modele bigram"
      ],
      "metadata": {
        "id": "s4kPW1qUBOLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_bigram(previous_text, word_to_index, index_to_word, index_to_word_unigram, matrix, nb_words_after=20, no_repetition=True):\n",
        "    \n",
        "    #1. tokenisation de la phrse en entree\n",
        "    used_words = return_token(previous_text)\n",
        "\n",
        "    #2. Recuperons les deux dernier mot de la demie phrase\n",
        "    if len(used_words)<1: \n",
        "      last_bigram = \"START \" + used_words[-1]\n",
        "    else:\n",
        "      last_bigram = used_words[-2] + \" \" + used_words[-1]  \n",
        "\n",
        "    print(previous_text, end=\" \")\n",
        "\n",
        "    index_last_bigram = word_to_index[last_bigram]\n",
        "\n",
        "    for i in range(nb_words_after):  \n",
        "        \n",
        "        index_next_word, occurence = get_next_word(index_last_bigram, matrix)\n",
        "                \n",
        "        if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word_unigram[index_next_word])\n",
        "          print(index_to_word_unigram[index_next_word], end = ' ') # j'affiche donc au fur et à mesure sans aller à la ligne\n",
        "        else:\n",
        "            break;\n",
        "\n",
        "        index_last_bigram = word_to_index[used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]] # juste pour mettre a jour la variable contenant l'id du dernier bigram, celle qu'on repasse a la fonction permettant d'avoir le successeur probable\n",
        "            "
      ],
      "metadata": {
        "id": "iXnpWqZkwlMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx, occurence = get_next_word(word_to_index_bigram['suis pas'], bigram_matrix)\n",
        "print(index_to_word_unigram[idx])\n",
        "print(occurence)"
      ],
      "metadata": {
        "id": "oxZhynzE1Vzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_bigram(\"solde (espace)\", \n",
        "                    word_to_index_bigram, \n",
        "                    index_to_word_bigram, \n",
        "                    index_to_word_unigram, \n",
        "                    bigram_matrix,\n",
        "                   nb_words_after=100\n",
        "                   )"
      ],
      "metadata": {
        "id": "Kbdrvin811kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'efficacite des bigrams peut etre augmentee en passant aux trigrams, testons ce modele aussi\n",
        " Cest a dire on considere pour sequence de mots (trois mots)"
      ],
      "metadata": {
        "id": "j4nvUe0f3KIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRIGRAMS**"
      ],
      "metadata": {
        "id": "CjaJNVP03KIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout d'abord definissons d'abor les dictionnaires comme dans la methode bigram"
      ],
      "metadata": {
        "id": "YxdC4b3N3KIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_word_index_trigram(texts):\n",
        "    word_to_index = {\n",
        "    \"START START START\":0,\n",
        "    \"END\":1\n",
        "    }\n",
        "    index_to_word = ['START START START', 'END']\n",
        "\n",
        "    for speech in texts:\n",
        "        for i in range(len(speech)):\n",
        "            if i == 0:\n",
        "                word = 'START START ' + speech[i]\n",
        "            elif i==1:\n",
        "                word = 'START ' + speech[i-1] + ' ' + speech[i]\n",
        "            else:\n",
        "                word = speech[i-2] + ' ' + speech[i-1] + ' ' + speech[i]\n",
        "\n",
        "            #print(word)\n",
        "            if word not in index_to_word:\n",
        "                word_to_index[word] = len(index_to_word)\n",
        "                index_to_word.append(word)\n",
        "            \n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "def create_trigram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts):\n",
        "    V = len(index_to_word)\n",
        "    V_unigram = len(word_to_index_unigram)\n",
        "    matrix = np.zeros((V, V_unigram))\n",
        "\n",
        "    for sentence in texts:\n",
        "        for i in range(len(sentence)):\n",
        "            if i == 0:\n",
        "                matrix[word_to_index['START START START'], word_to_index_unigram[sentence[0]]] += 1 \n",
        "                if len(sentence)>1:                \n",
        "                  matrix[word_to_index['START START '+sentence[0]], word_to_index_unigram[sentence[i+1]]] += 1 \n",
        "\n",
        "            elif i ==1:\n",
        "              if len(sentence)>2:\n",
        "                matrix[word_to_index['START '+sentence[i-1] + ' ' +sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
        "\n",
        "            elif i >= (len(sentence)-1):\n",
        "                matrix[word_to_index[sentence[i-2] +' '+ sentence[i-1]+' '+sentence[i]], 1] += 1\n",
        "       \n",
        "            else: \n",
        "                matrix[word_to_index[sentence[i-2]+' '+sentence[i-1]+' '+sentence[i]], word_to_index_unigram[sentence[i+1]]] += 1\n",
        "    \n",
        "    return matrix"
      ],
      "metadata": {
        "id": "a65i4dej3KIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word to index and index to word for trigrams\n",
        "word_to_index_trigram, index_to_word_trigram = create_word_index_trigram(corpus)\n",
        "\n",
        "#Get trigram matrix\n",
        "trigram_matrix = create_trigram_matrix(word_to_index_trigram, index_to_word_trigram, word_to_index_unigram, corpus)\n",
        "\n",
        "index_to_word_trigram[:20]"
      ],
      "metadata": {
        "id": "sIVv8L0U3KIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generons maintenant le texte avec le modele trigram"
      ],
      "metadata": {
        "id": "Ud4iSJrc3KIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_trigram(previous_text, word_to_index, index_to_word, index_to_word_unigram, matrix, nb_words_after=20, no_repetition=True):\n",
        "\n",
        "    #1. tokenisation de la phrse en entree\n",
        "    used_words = return_token(previous_text)\n",
        "\n",
        "    #2. Recuperons les trois derniers mot de la demie phrase\n",
        "    if len(used_words)<2: \n",
        "      last_trigram = \"START \" + used_words[-1]\n",
        "    elif len(used_words)<3:\n",
        "      last_trigram = \"START \" + used_words[-2] + \" \" + used_words[-1]  \n",
        "    else:\n",
        "      last_trigram = used_words[-3] + \" \" + used_words[-2] + \" \" + used_words[-1] \n",
        "    \n",
        "    print(previous_text, end=\" \")\n",
        "\n",
        "    index_last_trigram = word_to_index[last_trigram]\n",
        "\n",
        "    for i in range(nb_words_after):  \n",
        "        \n",
        "        index_next_word, occurence = get_next_word(index_last_trigram, matrix)\n",
        "\n",
        "        if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word_unigram[index_next_word])\n",
        "          print(index_to_word_unigram[index_next_word], end = ' ') # j'affiche donc au fur et à mesure sans aller à la ligne\n",
        "        else:\n",
        "            break;    \n",
        "        \n",
        "        index_last_trigram = word_to_index[used_words[len(used_words)-3] + ' ' + used_words[len(used_words)-2] + ' ' + used_words[len(used_words)-1]]\n",
        "            "
      ],
      "metadata": {
        "id": "zEvJXOLX3KIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_trigram(\"Hors l'établissement d'une\", \n",
        "                    word_to_index_trigram, \n",
        "                    index_to_word_trigram, \n",
        "                    index_to_word_unigram, \n",
        "                    trigram_matrix,\n",
        "                   nb_words_after=20\n",
        "                   )"
      ],
      "metadata": {
        "id": "AnPyXfVC3KIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant essayons d'ecrire un algorithme qui prendra en parametre la taille n des grams"
      ],
      "metadata": {
        "id": "BUfA7v5K_CuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N GRAMS**"
      ],
      "metadata": {
        "id": "Tr4ESkUDRDx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation des deux dictionnaire et liste"
      ],
      "metadata": {
        "id": "CTri2kl27nh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "def create_word_index_ngram(texts, ngram_size = 3): # La fonction prend dorenavant un parametre precisant la taille des grams (initialisee à 3)\n",
        "    first_chaine = \"\" # Comme il faut creer une chaine de START de taille ngramsize, je l'initialise\n",
        "    for _ in range(ngram_size):\n",
        "      first_chaine = first_chaine + \"START \"\n",
        "    first_chaine = first_chaine[:-1] # j'enleve le dernier caractere qui est l'espace, cest la chaine START START START...  qui a l'id 0\n",
        "    word_to_index = {\n",
        "    first_chaine:0,\n",
        "    \"END\":1 # et END a l'id 1\n",
        "    }\n",
        "    index_to_word = [first_chaine, 'END'] # J'initialise la liste des mots\n",
        "\n",
        "    for speech in texts: # je parcours chaque phrase de mon corpus\n",
        "        for i in range(len(speech)): # pour chaque mot de la phrse, non pas mot mais token\n",
        "            word = \"\" # variable qui va contenir la sequence des mots, la sequence a la taille de ngramsize\n",
        "            if i < ngram_size -1 : # Dans cette condition je constitue la sequence, et ici specifiquement elle commence par START\n",
        "                word = 'START ' * (ngram_size - i - 1)\n",
        "                for k in range(i+1): # Je concatene donc le debut des start la avec les tokens suivants\n",
        "                    word = word + speech[k] + ' '\n",
        "            else: # Et ici je constitue les sequences qui ne commencent pas par START, genre ce sont les mots du milieu de la phrase, pas du debut\n",
        "                for k in range(i-(ngram_size-1), i+1):\n",
        "                    word = word + speech[k] + ' '\n",
        "\n",
        "            word = word[:-1] # j'enleve l'espace de la fin\n",
        "            if word not in index_to_word: # si la sequence n'est pas encore dans la liste je vais l'ajouter, a la derniere ligne\n",
        "                word_to_index[word] = len(index_to_word) # et je l'ajoute aussi dans le dictionnaire avec son id est son emplacement  dans la liste des sequences\n",
        "                index_to_word.append(word)\n",
        "            \n",
        "    return word_to_index, index_to_word # Je retourne le dictionnaire et la liste..."
      ],
      "metadata": {
        "id": "ZGhhf9wFRKsv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation de la matrice n-gram"
      ],
      "metadata": {
        "id": "7EHrleK974bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "# Cette fonction aussi prend le ngramsize en parametre et cela doit etre le meme que celui qui a ete utilisé dans create_word_index_ngram\n",
        "def create_ngram_matrix(word_to_index, index_to_word, word_to_index_unigram, texts, ngram_size = 3):\n",
        "    V = len(index_to_word) \n",
        "    V_unigram = len(word_to_index_unigram)\n",
        "    matrix = np.zeros((V, V_unigram)) # Je definis ma matrice, qui est de taille (nombre de sequence de mots * nombre de mots unigrams)\n",
        "\n",
        "    first_chaine = \"\" # Je fabrique la meme premiere chaine la que je vais ajouter a la colonne 0 de la matrice\n",
        "    for _ in range(ngram_size):\n",
        "      first_chaine = first_chaine + \"START \"\n",
        "    first_chaine = first_chaine[:-1]\n",
        "\n",
        "    for sentence in texts:\n",
        "        for i in range(len(sentence)): # Je parcours donc chaque token de la liste des tokens de la phrase en question\n",
        "            if i < ngram_size-1: # Dans cette condition je verifie que le token actuel est parmi les premiers tokens de la phrase, cest a dire pour les cas ou il n'a pas tous les token precedent s pour avoir la sequence complete à ettre dans la matrice\n",
        "                taille_start = \"\"\n",
        "                if i == 0: # a chaque fois je vais mettre la chaine START START ... dans la premiere colonne de la matrice\n",
        "                    matrix[word_to_index[first_chaine], word_to_index_unigram[sentence[i]]] += 1 # et mettre cette case à 1 pour marquer le debut d'une nouvelle phrase\n",
        "                if len(sentence) > i+1: # si la phrse est asseez longue, je mets la suite des sequences (ici ce sont les sequences qui commencent par START )\n",
        "                    # je fabrique la chaine qui commence par START à faire correspondre en ligne dans la matrice puis je concatene avec les premiers mots et je fais la correspondance dans la matrice cad chercher son successeur\n",
        "                    taille_start = \"START \" * (ngram_size-i-1) \n",
        "                    for j in range(i+1):\n",
        "                        taille_start = taille_start + sentence[j] + \" \"\n",
        "                    matrix[word_to_index[taille_start[:-1]], word_to_index_unigram[sentence[i+1]]] += 1  # j'incremente donc la case correspondant a la sequence et au successeur\n",
        "\n",
        "            elif i >= (len(sentence)-1): # Il s'agit ici du cas ou je suis au dernier caractere de la phrase\n",
        "                # je fabrique la chaine dont je cherche a mettre le successeur à END et la taille de la chaine est en fonction de la taille du ngram\n",
        "                taille_chaine = \"\"\n",
        "                for l in range(i-(ngram_size-1), i+1):\n",
        "                    taille_chaine = taille_chaine + sentence[l] + \" \"\n",
        "                matrix[word_to_index[taille_chaine[:-1]], 1] += 1 # je met la colonne de END à 1\n",
        "            else: # Il s'agit donc du cas où je suis au milieur de la phrase, quand je tombe sur un mot (ou token), je prends simplement ses precedents, autant qu'il en faut pour faire correspondre a la taille de ngramsize\n",
        "                taille_chaine = \"\" \n",
        "                for l in range(i-(ngram_size-1), i+1): # Je concatene ce mot et ses precedents\n",
        "                    taille_chaine = taille_chaine + sentence[l] + \" \"\n",
        "                matrix[word_to_index[taille_chaine[:-1]], word_to_index_unigram[sentence[i+1]]] += 1 # Puis j'increment la case correspondant e avec le sucesseur en colonne \n",
        "    \n",
        "    return matrix    # Je retourne ensuite la matrice, toute faite"
      ],
      "metadata": {
        "id": "ClKluXP8aY6m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appelons maintenant ces fonctions pour constituer notre matrice (le noeud de la chose)"
      ],
      "metadata": {
        "id": "1DesNDbdXwhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get word to index and index to word for trigrams\n",
        "word_to_index_ngram, index_to_word_ngram = create_word_index_ngram(corpus, 2)\n",
        "\n",
        "#Get ngram matrix\n",
        "ngram_matrix = create_ngram_matrix(word_to_index_ngram, index_to_word_ngram, word_to_index_unigram, corpus, 2)\n",
        "\n",
        "index_to_word_ngram[:20]"
      ],
      "metadata": {
        "id": "v-YuPLWPfPLg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme la methode de detection du mot suivant est deja fonctionnelle, implementons la fonction permettant de generer la suite de la phrase"
      ],
      "metadata": {
        "id": "nVhm_ELJf2J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"Obligatoire\"\"\"\n",
        "\n",
        "def get_sentence_ngram(previous_text, nb_words_after=100):\n",
        "\n",
        "    #1. tokenisation de la phrase en entree\n",
        "    used_words = return_token(previous_text)\n",
        "    print(len(used_words))\n",
        "\n",
        "    #2. Recuperons les n derniers mot de la demie phrase si n<10 et n sinon\n",
        "    \n",
        "    last_ngram = \"\"\n",
        "\n",
        "    if len(used_words) == 0: # S'il n'entre rien genre une chaine vide, je lui retourne ce message d'erreur\n",
        "        return \"Entrez un texte avec au moins un caractere.\"\n",
        "\n",
        "    elif len(used_words) < 10: # Cas ou la demie phrase a plus de 10 caracteres, je considere ngram comme une sequence de len(used_words)\n",
        "      ngram_size = len(used_words) # la taille des sequences à former est simplement la taille de la demie phrase entrée\n",
        "      \n",
        "      for h in range(ngram_size, 0, -1):\n",
        "          last_ngram = last_ngram + used_words[-h] + \" \"\n",
        "\n",
        "    else : # Cas ou la demie phrase a plus de 10 caracteres, je considere ngram comme une sequence de 10\n",
        "      \n",
        "      ngram_size = 10 # la taille des sequences à former est simplement 10\n",
        "      for h in range(ngram_size, 0, -1):\n",
        "          last_ngram = last_ngram + used_words[-h] + \" \"\n",
        "\n",
        "    # 3. Fabriquons maintenant les dictionnaires, la liste des sequences et la matrice en fonction de la taille des ngrams, \n",
        "\n",
        "    #Get word to index and index to word for unigrams\n",
        "    word_to_index_unigram, index_to_word_unigram = create_word_index_unigram(corpus)\n",
        "\n",
        "    #Get word to index and index to word for ngrams\n",
        "    word_to_index_ngram, index_to_word_ngram = create_word_index_ngram(corpus, ngram_size)\n",
        "\n",
        "    #Get ngram matrix\n",
        "    matrix = create_ngram_matrix(word_to_index_ngram, index_to_word_ngram, word_to_index_unigram, corpus, ngram_size)\n",
        "    \n",
        "    last_ngram = last_ngram[:-1] # Pour enlever l'espace de la fin\n",
        "\n",
        "    result = previous_text + \" \"\n",
        "\n",
        "    index_last_ngram = word_to_index_ngram[last_ngram]\n",
        "\n",
        "    for i in range(nb_words_after):  \n",
        "        \n",
        "        index_next_word, occurence = get_next_word(index_last_ngram, matrix)\n",
        "\n",
        "        if(index_next_word!=1):        # Je verifie que le mot que je veux ajouter n'est pas le mot END car cest la fin de la phrase, si cest le cas, je ne l'ajoute juste pas\n",
        "          used_words.append(index_to_word_unigram[index_next_word])\n",
        "          result = result + index_to_word_unigram[index_next_word] + \" \"\n",
        "        else:\n",
        "            break;    \n",
        "\n",
        "        last_ngram = \"\"\n",
        "        for h in range(ngram_size, 0, -1):\n",
        "          last_ngram = last_ngram + used_words[-h] + \" \"    \n",
        "        index_last_ngram = word_to_index_ngram[last_ngram[:-1]]\n",
        "    return result[:-1]"
      ],
      "metadata": {
        "id": "2sv8OZxogGby"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentence_ngram(\"Merci de consulter vos mails et messages spams,\", 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "XEokCsouHGM6",
        "outputId": "6316d08f-4eea-4b24-e0cd-0fe2d88345b8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Merci de consulter vos mails et messages spams, nous vous avons fais parvenir la fiche de demande de service , bien vouloir l' imprimer , le remplir convenablement(en vous assurant de cocher uniquement le service C - Online , déblocage compte et aussi réinitialisation mot de passe ) . Accompagnez ce document de votre copie de passeport sur laquelle vous signez 03 fois . Tous vos documents sous le format PDF s' il vous plaît .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}